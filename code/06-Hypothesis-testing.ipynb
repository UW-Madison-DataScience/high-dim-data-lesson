{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923c7ea7-fc14-4b16-a21e-519f7f748aff",
   "metadata": {},
   "source": [
    "---\n",
    "title: Hypothesis testing\n",
    "teaching: 45\n",
    "exercises: 2\n",
    "keypoints:\n",
    "- \"All models are wrong, but some are useful.\"\n",
    "- \"Before reading into a model's estimated coefficients, modelers must take care to check for evidence of overfitting and assess the 5 assumptions of linear regression.\"\n",
    "- \"One-hot enoding, while necesssary, can often produce very sparse binary predictors which have little information. Predictors with very little variability should be removed prior to model fitting.\"\n",
    "objectives:\n",
    "- \"Understand how to assess the validity of a multivariate regression model.\" \n",
    "- \"Understand how to use statistics to evaluate the likelihood of existing relationships recovered by a multivariate model.\"\n",
    "questions:\n",
    "- \"How can multivariate models be used to detect interesting relationships found in nature?\"\n",
    "- \"What are the assumptions of linear regression models?\"\n",
    "- \"How can we rigorously evaluate the validity and accuracy of a multivariate regression model?\"\n",
    "- \"How should we preprocess categorical predictors and sparse binary predictors?\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879cf75-a2e5-4148-a4f6-03f45994af8d",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "We will ultimately use hypothesis testing to determine if our model has found any statistically signficant relationships. What does it mean to be statistically signficiant? It means that an observed relationship is unlikely (< 5% chance if p=.005) to occur due to chance alone. \n",
    "\n",
    "To run statistics on a regression model, we start with two hypotheses — one null and one alternative.\n",
    "* $H_0$ (Null hypothesis): $m$ = 0 (i.e., slope is flat)\n",
    "* $H_A$ (Alternative hypothesis): $m \\neq 0$ (i.e.., slope is not completely flat) \n",
    "\n",
    "In other words, we are testing to see if a predictor has a consistent effect on some target variable. We are NOT testing the magnitidute of the effect (we will discuss effect sizes later); simply whether or not an observed effect is due to chance or not. In statistics, we start with the null hypothesis as our default and review evidence (the fitted model and its estimated parameters and error measurement) to see if the observed data suggests that the null hypothesis should be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565cf8f-8e26-4e48-a18c-c61a3dbc7c62",
   "metadata": {},
   "source": [
    "### Overview of hypothesis testing procedure\n",
    "The procedure for testing whether predictor(s) have a statistically significant effect on a target variable in a regression model typically involves the following steps:\n",
    "\n",
    "1. **Formulate the null hypothesis (H₀) and alternative hypothesis (H₁) for the test.** The null hypothesis typically states that the predictor has no effect on the response variable (coef=0), while the alternative hypothesis suggests that there is a significant effect (coef!=0).\n",
    "\n",
    "2. **If using multiple predictors, check for multicollinearity.** Multicollinearity can be an especially pervasive.\n",
    "\n",
    "3. **Fit the regression model to your data.** Obtain the estimated coefficients for each predictor, along with their standard errors.\n",
    "\n",
    "4. **Check for evidence of overfitting**: If overfitting occurs, it doesn't make much sense to make general claims about observed relationships in the model.\n",
    "\n",
    "4. **Evaluate linearity assumption**: If using a univariate model, can do this step before model fitting via a simple scatterplot.\n",
    "\n",
    "5. **Evaluate normality of errors assumption.**\n",
    "\n",
    "6. **Calculate the test statistic**: Calculate the test statistic based on the estimated coefficient and its standard error. The test statistic depends on the specific regression model and hypothesis being tested. Common test statistics include t-statistic, z-statistic, or F-statistic.\n",
    "\n",
    "6. **Determine the critical value**: Determine the critical value or significance level (α) at which you want to test the hypothesis. The significance level typically ranges from 0.01 to 0.05, depending on the desired level of confidence.\n",
    "\n",
    "7. **Compare the test statistic and critical value**: Compare the calculated test statistic with the critical value. If the test statistic falls within the critical region (i.e., the calculated p-value is less than the significance level), you reject the null hypothesis and conclude that the predictor is statistically significant. If the test statistic does not fall within the critical region, you fail to reject the null hypothesis, indicating that the predictor is not statistically significant.\n",
    "\n",
    "8. **Interpret the results**: Based on the conclusion from the hypothesis test, interpret the significance of the predictor. If the predictor is deemed statistically significant, it suggests that there is evidence of a relationship between the predictor and the response variable. If the predictor is not statistically significant, it implies that there is no significant evidence of an effect.\n",
    "\n",
    "It's important to note that significance tests provide statistical evidence for or against the null hypothesis, but they should be interpreted alongside other factors such as effect size, practical significance, and the context of the problem being studied. Additionally, it's crucial to consider the assumptions and limitations of the regression model and the underlying data when interpreting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fade5fe-7494-47fc-a69d-91ddcf8b5763",
   "metadata": {},
   "source": [
    "### 1. Specify hypotheses\n",
    "We begin by formulating the null hypothesis (H₀) and alternative hypothesis (H₁) for each predictor we intend to include in the model. The null hypothesis states that the predictor has no effect on the response variable, while the alternative hypothesis suggests that there is a significant effect (typically < 5% chance of observing the relationship by chance alone). Before we can reject the null hypothesis, we must make sure to satisfy all multivariate regression assumptions to ensure reliable and valid inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08693af4-2b56-4988-a6ab-b1ca74ddb5e5",
   "metadata": {},
   "source": [
    "### 7. Calculate the test statistic\n",
    "**t-statistic**: The t-statistic is typically used to test the statistical significance of individual coefficient estimates in the regression model. It measures the ratio of the estimated coefficient to its standard error. The t-test helps assess whether a specific predictor variable has a significant effect on the response variable while accounting for the uncertainty in the coefficient estimate.\n",
    "\n",
    "P-values for t-statistics are calculated based on the t-distribution. The t-distribution is a probability distribution that is used when the population standard deviation is unknown and needs to be estimated from the sample.\n",
    "\n",
    "To calculate the p-value for a t-statistic, you follow these general steps:\n",
    "\n",
    "1. Formulate the null hypothesis (H0) and alternative hypothesis (H1) for the test you are conducting.\n",
    "\n",
    "2. Calculate the t-statistic for the test using the formula:\n",
    "\n",
    "    t = (estimate - null_value) / standard_error\n",
    "    \n",
    "    where \"estimate\" is the estimated coefficient or difference, \"null_value\" is the value specified under the null hypothesis (often 0), and \"standard_error\" is the standard error of the coefficient or difference estimate.\n",
    "\n",
    "4. Determine the degrees of freedom (df) for the t-distribution. In simple linear regression, the degrees of freedom are typically n - 2, where n is the number of observations. In multivariate regression, the degrees of freedom depend on the number of predictors and the sample size.\n",
    "\n",
    "5. Look up the p-value associated with the calculated t-value and degrees of freedom in the t-distribution table or use statistical software to calculate it. The p-value represents the probability of observing a t-value as extreme as, or more extreme than, the calculated value under the null hypothesis.\n",
    "\n",
    "6. Compare the p-value to the predetermined significance level (commonly 0.05). If the p-value is less than the significance level, you reject the null hypothesis in favor of the alternative hypothesis. If the p-value is greater than or equal to the significance level, you fail to reject the null hypothesis.\n",
    "\n",
    "By calculating the p-value for the t-statistic, you can assess the statistical significance of the coefficient estimate or the difference being tested. A lower p-value indicates stronger evidence against the null hypothesis and suggests a more significant relationship or effect.\n",
    "\n",
    "#### The more manual route of calculating p-values... \n",
    "\n",
    "In this code, after fitting the multivariate regression model and obtaining the coefficient estimates in the coefs Series and the standard errors in the std_errs Series, we calculate the t-values by dividing the coefficient estimates by the standard errors. The t-value represents the ratio of the estimated coefficient (or difference) to its standard error. It measures the number of standard errors by which the estimated coefficient differs from zero. The standard error reflects the precision of the estimated coefficient, and a larger t-value indicates a larger difference relative to the standard error.\n",
    "\n",
    "Next, we use the t-values to calculate the two-sided p-values using the stats.t.sf function from the SciPy library. The np.abs(t_values) ensures that we consider the absolute values of the t-values to calculate the p-values for both positive and negative t-values. We multiply the resulting p-values by 2 to obtain the two-sided p-values. The p-value is the probability of observing a t-value as extreme as, or more extreme than, the one calculated, assuming the null hypothesis is true. By convention, if the p-value is smaller than a predetermined significance level (commonly 0.05), we reject the null hypothesis in favor of the alternative hypothesis, indicating that the coefficient is statistically significant.\n",
    "\n",
    "Dividing the t-value by the standard error yields a test statistic that follows a t-distribution with degrees of freedom determined by the sample size and the complexity of the model. By looking up the p-value associated with the calculated test statistic in the t-distribution table or using statistical software, we can determine the probability of observing such an extreme or more extreme value. This probability is the p-value.\n",
    "\n",
    "In summary, dividing the t-value by the standard error is a way to standardize the coefficient estimate and quantify its significance relative to its precision. The resulting test statistic follows a t-distribution, and by calculating the associated p-value, we can assess the statistical significance of the coefficient estimate and make inference about its relationship with the predictor variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b68a89-6caf-4ccd-9bc8-189383a34bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Get the coefficient estimates and standard errors\n",
    "coefs = trained_model.params\n",
    "std_errs = trained_model.bse\n",
    "\n",
    "# Calculate the t-values and p-values\n",
    "t_values = coefs / std_errs\n",
    "p_values = stats.t.sf(np.abs(t_values), df=trained_model.df_resid) * 2\n",
    "p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30650360-b901-4cb0-ba44-c8f9a3ecb260",
   "metadata": {},
   "source": [
    "Note: while our tests show that all predictors are significant, we can't rely on these results since the linearity assumption was violated with this model/dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da40b1-8228-4aa8-b8f3-88d3976b3c0f",
   "metadata": {},
   "source": [
    "#### Quicker route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4439520-6e17-4c40-bef5-ae56cd7ca76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = trained_model.pvalues\n",
    "print(p_values)\n",
    "\n",
    "trained_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
