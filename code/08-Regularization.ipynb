{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704f6c52-3d0e-4cd7-bc4f-e39c092a4c63",
   "metadata": {},
   "source": [
    "---\n",
    "title: Regularization methods - lasso, ridge, and elastic net\n",
    "teaching: 45\n",
    "exercises: 2\n",
    "keypoints:\n",
    "- \"\"\n",
    "objectives:\n",
    "- \"\"\n",
    "questions:\n",
    "- \"How can LASSO regularization be used as a feature selection method?\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c0f4e-e318-4210-9e0f-40f7f448c19a",
   "metadata": {},
   "source": [
    "## Introduction to the LASSO Model in high-dimensional data analysis\n",
    "In the realm of high-dimensional data analysis, where the number of predictors begins to approach or exceed the number of observations, traditional regression methods can become challenging to implement and interpret. The Least Absolute Shrinkage and Selection Operator (LASSO) offers a powerful solution to address the complexities of high-dimensional datasets. This technique, introduced by Robert Tibshirani in 1996, has gained immense popularity due to its ability to provide both effective prediction and feature selection.\n",
    "\n",
    "The LASSO model is a regularization technique designed to combat overfitting by adding a penalty term to the regression equation. The essence of the LASSO lies in its ability to shrink the coefficients of less relevant predictors towards zero, effectively \"shrinking\" them out of the model. This not only enhances model interpretability by identifying the most important predictors but also reduces the risk of multicollinearity and improves predictive accuracy.\n",
    "\n",
    "LASSO's impact on high-dimensional data analysis is profound. It provides several benefits:\n",
    "\n",
    "* Feature Selection / Interpretability: The LASSO identifies and retains the most relevant predictors. With a reduced set of predictors, the model becomes more interpretable, enabling researchers to understand the driving factors behind the predictions.\n",
    "\n",
    "* Regularization / Dimensionality Reduction: The L1 penalty prevents overfitting by constraining the coefficients, even in cases with a large number of predictors. The L1 penality inherently reduces the dimensionality of the model, making it suitable for settings where the number of predictors is much larger than the sample size.\n",
    "\n",
    "* Improved Generalization: Related to the above point, LASSO's feature selection capabilities contribute to better generalization and prediction performance on unseen data.\n",
    "\n",
    "* Data Efficiency: LASSO excels when working with limited samples, offering meaningful insights despite limited observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e286a602-deb9-4a4e-876b-5b6bdc052638",
   "metadata": {},
   "source": [
    "### The L1 penalty\n",
    "The key concept behind the LASSO is its use of the L1 penalty, which is defined as the sum of the absolute values of the coefficients (parameters) of the model, multiplied by a regularization parameter (usually denoted as λ or alpha).\n",
    "\n",
    "In the context of linear regression, the L1 penalty can be incorporated into the ordinary least squares (OLS) loss function as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a51a96-6be3-437f-8634-b30a4835bf70",
   "metadata": {},
   "source": [
    "![LASSO Model](https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Lasso.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621086fc-3ee6-4e59-9731-38bc80be635f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Where:\n",
    "\n",
    "* λ (lambda) is the regularization parameter that controls the strength of the penalty. Higher values of λ lead to stronger regularization and more coefficients being pushed towards zero.\n",
    "* βi is the coefficient associated with the i-th predictor.\n",
    "\n",
    "The L1 penalty has a unique property that it promotes sparsity. This means that it encourages some coefficients to be exactly zero, effectively performing feature selection. In contrast to the L2 penalty (Ridge penalty), which squares the coefficients and promotes small but non-zero values, the L1 penalty tends to lead to sparse solutions where only a subset of predictors are chosen. As a result, the LASSO automatically performs feature selection, which is especially advantageous when dealing with high-dimensional datasets where many predictors may have negligible effects on the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec18376-3036-4b5f-8430-381882f299fe",
   "metadata": {},
   "source": [
    "### Fit lasso model and run stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a1da1-dd65-4795-a41d-9ad2ffc7454a",
   "metadata": {},
   "source": [
    "### Fit lasso model and run stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10aada6e-8868-42c6-a017-924adf3f97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "housing = fetch_openml(name=\"house_prices\", as_frame=True, parser='auto') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f12bd0-f653-46a1-8a75-4fc0d4227014",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ames_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the Ames housing dataset (replace with your own data loading code)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ames_data = pd.read_csv('ames_housing.csv')\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Define predictors and target variable\u001b[39;00m\n\u001b[0;32m     11\u001b[0m predictors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLotArea\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYearBuilt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYearRemodAdd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGarageArea\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGarageCars\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 12\u001b[0m X \u001b[38;5;241m=\u001b[39m ames_data[predictors]\n\u001b[0;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m ames_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ames_data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the Ames housing dataset (replace with your own data loading code)\n",
    "# ames_data = pd.read_csv('ames_housing.csv')\n",
    "\n",
    "# Define predictors and target variable\n",
    "predictors = ['LotArea', 'YearBuilt', 'YearRemodAdd', 'GarageArea', 'GarageCars']\n",
    "X = ames_data[predictors]\n",
    "y = ames_data['SalePrice']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create LassoCV model with cross-validation for lambda selection\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-6, 6, 13), cv=5)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Calculate p-values for LASSO coefficients\n",
    "X_train_with_constant = sm.add_constant(X_train)\n",
    "lasso_model = sm.OLS(y_train, X_train_with_constant)\n",
    "lasso_results = lasso_model.fit_regularized(alpha=lasso_cv.alpha_, L1_wt=1.0)\n",
    "\n",
    "# Print the summary of LASSO results\n",
    "print(lasso_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af14d4",
   "metadata": {
    "id": "28af14d4"
   },
   "source": [
    "### Split data into train/test sets and zscore\n",
    "We will now split our data into two separate groupings — one for fitting or training the model (\"train set\") and another for testing (\"test set\") the model's ability to generalize to data that was excluded during training. The amount of data you exclude for the test set should be large enough that the model can be vetted against a diverse range of samples. A common rule of thumb is to use 3/4 of the data for training, and 1/3 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c58c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b7c58c0",
    "outputId": "d85b69f2-db8b-499a-bbfb-021e14344a27"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.33, \n",
    "                                                    random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(type(y_train))\n",
    "print(type(X_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-GERuBw8iALC",
   "metadata": {
    "id": "-GERuBw8iALC"
   },
   "outputs": [],
   "source": [
    "# sklearn version\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# stats model version (for hypothesis testing)\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "def train_linear_model(X_train, y_train, model_type):\n",
    "    if model_type == \"unregularized\":\n",
    "        reg = LinearRegression().fit(X_train,y_train)\n",
    "#         reg = ols(\"dist ~ speed\", data=cars).fit()\n",
    "    else:\n",
    "        raise ValueError('Unexpected model_type encountered; model_type = ' + model_type)\n",
    "  \n",
    "    # print number of estimated model coefficients. Need to add one to account for y-intercept (not included in reg.coef_ call)\n",
    "    print('# model coefs = ' + str(len(reg.coef_)+1))\n",
    "\n",
    "    return reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1f6fa-2c67-4491-9134-e0509045c19d",
   "metadata": {},
   "source": [
    "Define a function `measure_model_err` to help us measure the model's performance (train/test RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e481b-bc04-4d07-9066-7064762e9856",
   "metadata": {},
   "source": [
    "Define a function `fit_eval_model` that will call both `train_linear_model` and `measure_model_err` and report back on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZCX7Kk2z3Ftw",
   "metadata": {
    "id": "ZCX7Kk2z3Ftw"
   },
   "source": [
    "## Fit multivariate model using all predictor vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f821b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac1f821b",
    "outputId": "cb5e5943-a268-4809-e25e-5c12ae07df97"
   },
   "outputs": [],
   "source": [
    "help(fit_eval_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Id0j91sEBgxj",
   "metadata": {
    "id": "Id0j91sEBgxj"
   },
   "source": [
    "## Regularized regression: ridge, lasso, elastic net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58W5L2_Gw3GH",
   "metadata": {
    "id": "58W5L2_Gw3GH"
   },
   "source": [
    "### Ridge and RidgeCV\n",
    "- Show ridge optimization equation\n",
    "- Default CV is Leave-One-Out. In this form of CV, all samples in the data except for one are used as the inital training set. The left out sample is used a validation set.\n",
    "- One alpha value used for entire model; larger alphas give more weight to the penalty/regularization term of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "luCE0ZniCtF1",
   "metadata": {
    "id": "luCE0ZniCtF1"
   },
   "source": [
    "Edit function below to use multiple regression techniques (add model_type input)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09dfec3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a09dfec3",
    "outputId": "e5008017-8b95-4110-fd50-d7f5888f582c"
   },
   "outputs": [],
   "source": [
    "# import sklearn's ridge model with built-in cross-validation\n",
    "from sklearn.linear_model import RidgeCV \n",
    "\n",
    "# fit model using multivariate_model_feats and ridge regression\n",
    "RMSE_train, RMSE_test = fit_eval_model(X_train, y_train, X_test, y_test, labels, 'ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fvp1TBfqTBiN",
   "metadata": {
    "id": "Fvp1TBfqTBiN"
   },
   "source": [
    "- What is the model's train and test error? How does this compare to the unregularized model we fit using all predictor variables? How does this model compare to the best univariate model we fit?\n",
    "  - The ridge model does much better (i.e., in terms of Test RMSE) than the unregularized model that uses all predictor vars.\n",
    "  - Unregularized_all_predictors_testRMSE: 3562241001\n",
    "  - Unregularized_best_univariate_testRMSE: 48243\n",
    "  - Regularized_all_predictors_testRMSE: 39004\n",
    "\n",
    "- What alpha value was selected using RidgeCV? Is it a lower or higher value? What does this value tell you about the model?\n",
    "  - This model is highly regularized/penalized since it has a large alpha value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zbw26P6XTKoT",
   "metadata": {
    "id": "Zbw26P6XTKoT"
   },
   "source": [
    "### LASSO\n",
    "- explain why there's a random state param in LASSO but not ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SqtdDGF_r19h",
   "metadata": {
    "id": "SqtdDGF_r19h"
   },
   "outputs": [],
   "source": [
    "# edit train_linear_model to train ridge models as well\n",
    "def train_linear_model(X_train, y_train, model_type):\n",
    "    if model_type == \"unregularized\":\n",
    "        reg = LinearRegression().fit(X_train,y_train)\n",
    "    elif model_type == 'ridge':\n",
    "        reg = RidgeCV(alphas=[1e-3,1e-2,1e-1,1,10,100,1000], store_cv_values=True).fit(X_train,y_train)\n",
    "        print(reg.cv_values_.shape) # num_datapoints x num_alphas\n",
    "        print(np.mean(reg.cv_values_, axis=0))\n",
    "        print(reg.alpha_)\n",
    "    elif model_type == 'lasso':\n",
    "        reg = LassoCV(random_state=0, alphas=[1e-3,1e-2,1e-1,1,10,100,1000], max_iter=100000, tol=1e-3).fit(X_train,y_train)\n",
    "        print(reg.alpha_)\n",
    "        print(reg.alphas_)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Unexpected model_type encountered; model_type = ' + model_type)\n",
    "\n",
    "    # print number of estimated model coefficients. Need to add one to account for y-intercept (not included in reg.coef_ call)\n",
    "    print('# model coefs = ' + str(len(reg.coef_)+1))\n",
    "\n",
    "    return reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wdVZs_vOwVfW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wdVZs_vOwVfW",
    "outputId": "8e3a21b6-4866-4fbb-c029-c9ad93da9f0d"
   },
   "outputs": [],
   "source": [
    "# import sklearn's lasso model with built-in cross-validation\n",
    "from sklearn.linear_model import LassoCV \n",
    "\n",
    "# fit model using multivariate_model_feats and ridge regression\n",
    "RMSE_train, RMSE_test = fit_eval_model(X_train, y_train, X_test, y_test, labels, 'lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0okG-D-LK4Cq",
   "metadata": {
    "id": "0okG-D-LK4Cq"
   },
   "source": [
    "Add elastic net option to function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aoEdgKzey9Ci",
   "metadata": {
    "id": "aoEdgKzey9Ci"
   },
   "outputs": [],
   "source": [
    "# edit train_linear_model to train ridge models as well\n",
    "def train_linear_model(X_train, y_train, model_type):\n",
    "    if model_type == \"unregularized\":\n",
    "        reg = LinearRegression().fit(X_train,y_train)\n",
    "    elif model_type == 'ridge':\n",
    "        reg = RidgeCV(alphas=[1e-3,1e-2,1e-1,1,10,100,1000], store_cv_values=True).fit(X_train,y_train)\n",
    "        print(reg.cv_values_.shape) # num_datapoints x num_alphas\n",
    "        print(np.mean(reg.cv_values_, axis=0))\n",
    "        print('alpha:', reg.alpha_)\n",
    "    elif model_type == 'lasso':\n",
    "        reg = LassoCV(random_state=0, alphas=[1e-3,1e-2,1e-1,1,10,100,1000], max_iter=100000, tol=1e-3).fit(X_train,y_train)\n",
    "        print('alpha:', reg.alpha_)\n",
    "        print('alphas:', reg.alphas_)\n",
    "    elif model_type == 'elastic':\n",
    "        reg = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],alphas=[1e-5,1e-4,1e-3,1e-2,1e-1,1,10]).fit(X_train,y_train)\n",
    "        print('alpha:', reg.alpha_)\n",
    "        print('l1_ratio:', reg.l1_ratio_)\n",
    "    else:\n",
    "        raise ValueError('Unexpected model_type encountered; model_type = ' + model_type)\n",
    "\n",
    "    # print number of estimated model coefficients. Need to add one to account for y-intercept (not included in reg.coef_ call)\n",
    "    print('# model coefs = ' + str(len(reg.coef_)+1))\n",
    "\n",
    "    return reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i0tAZQUwLQt6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0tAZQUwLQt6",
    "outputId": "54ed6deb-18b6-4216-def5-344a35b7a1ec",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# fit model using multivariate_model_feats and ridge regression\n",
    "RMSE_train, RMSE_test = fit_eval_model(X_train, y_train, X_test, y_test, labels, 'elastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6fdbfd",
   "metadata": {
    "id": "2b6fdbfd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5adc17",
   "metadata": {
    "id": "8d5adc17"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec7611",
   "metadata": {
    "id": "09ec7611"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efb25a",
   "metadata": {
    "id": "a5efb25a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5211781",
   "metadata": {
    "id": "e5211781"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a52d36",
   "metadata": {
    "id": "61a52d36"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf7bb9",
   "metadata": {
    "id": "3fcf7bb9"
   },
   "outputs": [],
   "source": [
    "# Diabetes dataset\n",
    "\n",
    "# from sklearn import datasets\n",
    "# example datasets from sklean: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes\n",
    "# iris_X, iris_y = datasets.load_iris(return_X_y=True)\n",
    "# more info on diabetes dataset: https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\n",
    "# diabetes = datasets.load_diabetes(return_X_y=False,as_frame=False)\n",
    "# print(type(diabetes))\n",
    "# feat_names=diabetes['feature_names']\n",
    "# print(feat_names)\n",
    "# data=diabetes['data']\n",
    "# target=diabetes['target'] # the target is a quantitative measure of disease progression one year after baseline\n",
    "# print(data.shape)\n",
    "# print(target.shape)\n",
    "# print(diabetes_X.shape) # 442 observations, 10 features\n",
    "# diabetes_y\n",
    "\n",
    "# California housing dataset\n",
    "\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "# housing = fetch_california_housing()\n",
    "# # housing\n",
    "# feat_names=housing['feature_names']\n",
    "# print(feat_names)\n",
    "# print(len(feat_names))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9uknZVyw5KO2",
    "83cff558",
    "6bb4f8a3"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
