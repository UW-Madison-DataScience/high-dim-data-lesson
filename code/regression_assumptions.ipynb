{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df05c1d-389a-46a6-94c0-0e693109c452",
   "metadata": {},
   "source": [
    "### Review of Univariate Linear Modeling: Goals, Assumptions, and Hypothesis Testing\n",
    "When fitting a linear model to data, researchers typically have one or both of the following goals in mind:\n",
    "1. Predict future or unseen values for the target variable (y). This form of modeling is typically referred to as *predictive modeling*.\n",
    "2. Infer if the trend observed in the model is statistically significant (i.e., the dependent variably \n",
    "\n",
    "#### Linear model equation\n",
    "Y = mX + B\n",
    "\n",
    "#### Hypotheses in linear modeling\n",
    "* H_0 (Null hypothesis): m = 0 (i.e., slope is flat)\n",
    "* H_A (Alternative hypothesis): m != 0 (i.e.., slope is not completely flat) \n",
    "\n",
    "#### The 4 Assumptions for Linear Regression Hypothesis Testing\n",
    "1. Linearity: There is a linear relation between Y and X - **linear plot of real vs predicted**\n",
    "    a. **Note**: In practice, linear models are often used to model nonlinear relationships due to complexity (number of model parameters/coefs that need to be estimated) of nonlinear models. When using a linear model to model nonlinear relationships, it usually best to use resulting model for predictive purposes only. \n",
    "2. Normality: The error terms (residuals) are normally distributed - **plot distribution and calc mean (should be near 0)**\n",
    "3. Homoscedasticity: The variance of the error terms is constant over all X values (homoscedasticity) - **Graphical Method: Firstly do the regression analysis and then plot the error terms against the predicted values( Yi^). If there is a definite pattern (like linear or quadratic or funnel shaped) obtained from the scatter plot then heteroscedasticity is present.**\n",
    "    - calculate residuals and show their distribution\n",
    "    - build an ad hoc plot to test normality using a qq-plot\n",
    "    - Shapiro-Wilk Test\n",
    "4. Independence: The error terms are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9a2fa-6738-4522-bd82-e9d9d5952dfb",
   "metadata": {},
   "source": [
    "### Links\n",
    "* https://www.kaggle.com/code/shrutimechlearn/step-by-step-assumptions-linear-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae67c9f-43f0-4125-ab23-7c6ebb5ea70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>HousePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  HousePrice  \n",
       "0     15.3  396.90   4.98        24.0  \n",
       "1     17.8  396.90   9.14        21.6  \n",
       "2     17.8  392.83   4.03        34.7  \n",
       "3     18.7  394.63   2.94        33.4  \n",
       "4     18.7  396.90   5.33        36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "\"\"\"\n",
    "Artificial linear data using the same number of features and observations as the\n",
    "Boston housing prices dataset for assumption test comparison\n",
    "\"\"\"\n",
    "linear_X, linear_y = datasets.make_regression(n_samples=boston.data.shape[0],\n",
    "                                              n_features=boston.data.shape[1],\n",
    "                                              noise=75, random_state=46)\n",
    "\n",
    "# Setting feature names to x1, x2, x3, etc. if they are not defined\n",
    "linear_feature_names = ['X'+str(feature+1) for feature in range(linear_X.shape[1])]\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['HousePrice'] = boston.target\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4fc338-6e59-413f-90fc-c2078f56f766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(boston.data))\n",
    "print(type(boston.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f954588d-a38d-48c3-9eaa-842e6015973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fitting the model\n",
    "boston_model = LinearRegression()\n",
    "boston_model.fit(boston.data, boston.target)\n",
    "\n",
    "# Returning the R^2 for the model\n",
    "boston_r2 = boston_model.score(boston.data, boston.target)\n",
    "print('R^2: {0}'.format(boston_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0a8ba8-9bda-415b-93ab-543c8c0f2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below function adapted from: https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/\n",
    "def calculate_residuals(model, features, label):\n",
    "    \"\"\"\n",
    "    Creates predictions on the features with the model and calculates residuals\n",
    "    \"\"\"\n",
    "    predictions = model.predict(features)\n",
    "    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n",
    "    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0408c0d5-4f8f-4426-8955-fd2a3d49fb0b",
   "metadata": {},
   "source": [
    "### Linearity (from: https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/)\n",
    "This assumes that there is a linear relationship between the predictors (e.g. independent variables or features) and the response variable (e.g. dependent variable or label). This also assumes that the predictors are additive.\n",
    "\n",
    "Why it can happen: There may not just be a linear relationship among the data. Modeling is about trying to estimate a function that explains a process, and linear regression would not be a fitting estimator (pun intended) if there is no linear relationship.\n",
    "\n",
    "What it will affect: The predictions will be extremely inaccurate because our model is underfitting. This is a serious violation that should not be ignored.\n",
    "\n",
    "**How to detect it**: Plots! If there is only one predictor, this is pretty easy to test with a scatter plot. Most cases aren’t so simple, so we’ll have to modify this by using a scatter plot to see our predicted values versus the actual values (in other words, view the residuals). Ideally, the points should lie on or around a diagonal line on the scatter plot.\n",
    "\n",
    "How to fix it: Either adding polynomial terms to some of the predictors or applying nonlinear transformations . If those do not work, try adding additional variables to help capture the relationship between the predictors and the label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf1b5a-87f7-4dd3-aff4-2e443690723f",
   "metadata": {},
   "source": [
    "#### Option 1 - using sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d095f-4e88-4ee7-bc77-ca8c7eff075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below function adapted from: https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/\n",
    "import seaborn as sns\n",
    "def linear_assumption(model, features, label):\n",
    "    \"\"\"\n",
    "    Linearity: Assumes that there is a linear relationship between the predictors and\n",
    "               the response variable. If not, either a quadratic term or another\n",
    "               algorithm should be used.\n",
    "    \"\"\"\n",
    "    print('Assumption 1: Linear Relationship between the Target and the Feature', '\\n')\n",
    "        \n",
    "    print('Checking with a scatter plot of actual vs. predicted.',\n",
    "           'Predictions should follow the diagonal line.')\n",
    "    \n",
    "    # Calculating residuals for the plot\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "    \n",
    "    # Plotting the actual vs predicted values\n",
    "    sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, size=7)\n",
    "        \n",
    "    # Plotting the diagonal line\n",
    "    line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
    "    plt.plot(line_coords, line_coords,  # X and y points\n",
    "             color='darkorange', linestyle='--')\n",
    "    plt.title('Actual vs. Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6fbc6-45ab-4649-8be3-9136d03f7d84",
   "metadata": {},
   "source": [
    "#### Option 2 - using statsmodel package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985da32-f7bf-4efa-aa17-07aedba3d06b",
   "metadata": {},
   "source": [
    "### QQ plots - model residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5fbbe-abd8-4ae5-a1b5-863aaf0172e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(model_result.resid, line='s');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
